@article{Bahdanau2014NeuralMT,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2014},
  volume={abs/1409.0473},
}

@inproceedings{Sukhbaatar2015EndToEndMN,
  title={End-To-End Memory Networks},
  author={Sainbayar Sukhbaatar and Arthur Szlam and Jason Weston and Rob Fergus},
  booktitle={Neural Information Processing Systems},
  year={2015},
}


@inproceedings{shaw2018self,
  title={Self-Attention with Relative Position Representations},
  author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2018},
}

@article{dufter2022position,
  title={Position information in transformers: An overview},
  author={Dufter, Philipp and Schmitt, Martin and Sch{\"u}tze, Hinrich},
  journal={Computational Linguistics},
  volume={48},
  number={3},
  pages={733--763},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{likhomanenko2021cape,
  title={Cape: Encoding relative positions with continuous augmented positional embeddings},
  author={Likhomanenko, Tatiana and Xu, Qiantong and Synnaeve, Gabriel and Collobert, Ronan and Rogozhnikov, Alex},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16079--16092},
  year={2021}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}



@inproceedings{sennrich2015neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2016",
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{liu2024exposing,
  title={Exposing attention glitches with flip-flop language modeling},
  author={Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{Weston2014MemoryN,
  author       = {Jason Weston and
                  Sumit Chopra and
                  Antoine Bordes},
  title        = {Memory Networks},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
}

@inproceedings{Dai2019TransformerXLAL,
  title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime G. Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
}

@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971},
}

@article{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Cant{\'o}n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
}

@article{Jiang2023Mistral7,
  title={Mistral 7B},
  author={Albert Qiaochu Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L'elio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.06825},
}

@article{wang2019r,
  title={R-transformer: Recurrent neural network enhanced transformer},
  author={Wang, Zhiwei and Ma, Yao and Liu, Zitao and Tang, Jiliang},
  journal={arXiv preprint arXiv:1907.05572},
  year={2019}
}

@inproceedings{neishi2019relation,
  title={On the relation between position information and sentence length in neural machine translation},
  author={Neishi, Masato and Yoshinaga, Naoki},
  booktitle={Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},
  year={2019}
}

@article{zhao2023length,
  title={Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding},
  author={Zhao, Liang and Feng, Xiaocheng and Feng, Xiachong and Qin, Bin and Liu, Ting},
  journal={arXiv preprint arXiv:2312.17044},
  year={2023}
}

@inproceedings{
press2021train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{haviv2022transformer,
    title = "Transformer Language Models without Positional Encodings Still Learn Positional Information",
    author = "Haviv, Adi  and
      Ram, Ori  and
      Press, Ofir  and
      Izsak, Peter  and
      Levy, Omer",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP",
    year = "2022",
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}

@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International conference on machine learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{eldan2023tinystories,
  title={Tinystories: How small can language models be and still speak coherent english?},
  author={Eldan, Ronen and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.07759},
  year={2023}
}

@article{merity2017regularizing,
  title={Regularizing and optimizing LSTM language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}

@inproceedings{
merity2016pointer,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2017},
}
